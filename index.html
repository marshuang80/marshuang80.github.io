<!doctype html><!-- This site was created with Wowchemy. https://www.wowchemy.com --><!-- Last Published: January 29, 2023 --><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.7.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=B612+Mono&family=B612:wght@400;700&family=Jura:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=B612+Mono&family=B612:wght@400;700&family=Jura:wght@400;700&display=swap" media=print onload='this.media="all"'><link rel=stylesheet href=/css/vendor-bundle.min.16f785cdb553c8c4431db6775122af35.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.af55899d8b924748b901d470aa7b0bd6.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><meta name=author content="Mars (Shih-Cheng) Huang"><meta name=description content="A highly-customizable Hugo academic resume theme powered by Wowchemy website builder."><link rel=alternate hreflang=en-us href=https://marshuang80.github.io/><link rel=canonical href=https://marshuang80.github.io/><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu687e722fa5af903774003502d32d2ddb_42108_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu687e722fa5af903774003502d32d2ddb_42108_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#1565c0"><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@wowchemy"><meta property="twitter:creator" content="@wowchemy"><meta property="twitter:image" content="https://marshuang80.github.io/media/icon_hu687e722fa5af903774003502d32d2ddb_42108_512x512_fill_lanczos_center_3.png"><meta property="og:site_name" content="Mars Huang"><meta property="og:url" content="https://marshuang80.github.io/"><meta property="og:title" content="Mars Huang"><meta property="og:description" content="A highly-customizable Hugo academic resume theme powered by Wowchemy website builder."><meta property="og:image" content="https://marshuang80.github.io/media/icon_hu687e722fa5af903774003502d32d2ddb_42108_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="og:updated_time" content="2022-10-24T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebSite","potentialAction":{"@type":"SearchAction","target":"https://marshuang80.github.io?q={search_term_string}","query-input":"required name=search_term_string"},"url":"https://marshuang80.github.io"}</script><script src=https://identity.netlify.com/v1/netlify-identity-widget.js></script>
<link rel=alternate href=/index.xml type=application/rss+xml title="Mars Huang"><title>Mars Huang</title></head><body id=top data-spy=scroll data-offset=70 data-target=#navbar-main class=page-wrapper data-wc-page-id=3976528693a0108357f4928017600865><script src=/js/wowchemy-init.min.ec9d49ca50e4b80bdb08f0417a28ed84.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Mars Huang</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Mars Huang</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about data-target=#about><span>About</span></a></li><li class=nav-item><a class=nav-link href=/#featured data-target=#featured><span>Featured</span></a></li><li class=nav-item><a class=nav-link href=/#publications data-target=#publications><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/#experiences data-target=#experiences><span>Experiences</span></a></li><li class=nav-item><a class=nav-link href=/#photos data-target=#photos><span>Photography</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class="nav-item d-none d-lg-inline-flex"><a class=nav-link href=https://twitter.com/MarsScHuang data-toggle=tooltip data-placement=bottom title="Follow me on Twitter" target=_blank rel=noopener aria-label="Follow me on Twitter"><i class="fab fa-twitter" aria-hidden=true></i></a></li><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><span class="js-widget-page d-none"></span><section id=about class="home-section wg-about-avatar"><div class=home-section-bg></div><div class=container><div id=profile><div class=avatar-wrapper><img class="avatar avatar-circle" src=/authors/admin/avatar_hu0647cedca97c4c8f561204cc83f5d37a_415564_150x150_fill_q75_lanczos_center.jpg alt="Mars (Shih-Cheng) Huang" width=150 height=150>
<span class=avatar-emoji>ðŸš€</span></div><div class=portrait-title><h2>Mars (Shih-Cheng) Huang</h2><h3>Ph.D. Candidate @</h3><h3><a href=https://www.stanford.edu/ target=_blank rel=noopener><span>Stanford University</span></a></h3></div><ul class=network-icon aria-hidden=true><li><a href=https://twitter.com/MarsScHuang target=_blank rel=noopener aria-label=twitter data-toggle=tooltip data-placement=top title="Follow me on Twitter"><i class="fab fa-twitter big-icon"></i></a></li><li><a href="https://scholar.google.com/citations?hl=en&user=RLMiaZUAAAAJ" target=_blank rel=noopener aria-label=graduation-cap><i class="fas fa-graduation-cap big-icon"></i></a></li><li><a href=https://github.com/marshuang80 target=_blank rel=noopener aria-label=github><i class="fab fa-github big-icon"></i></a></li><li><a href=https://www.linkedin.com/in/mschuang/ target=_blank rel=noopener aria-label=linkedin><i class="fab fa-linkedin big-icon"></i></a></li><li><a href=/uploads/resume.pdf target=_blank rel=noopener aria-label=cv><i class="ai ai-cv big-icon"></i></a></li></ul><div class="article-style pt-2 d-flex justify-content-center"><div class=bio-text><p>Hi there - I am a PhD candidate in Biomedical Informatics at Stanford University, studying artificial intelligence and clinical informatics. I am currently advised by <a href=https://ai.stanford.edu/~syyeung/ target=_blank rel=noopener>Serena Yeung</a>, <a href=https://profiles.stanford.edu/curtis-langlotz target=_blank rel=noopener>Curtis Langlotz</a>, <a href=https://profiles.stanford.edu/nigam-shah target=_blank rel=noopener>Nigam Shah</a> and previously by <a href="https://scholar.google.com/citations?user=z1UtMSYAAAAJ&hl=en" target=_blank rel=noopener>Matthew P. Lungren</a>. I am affiliated with Stanford&rsquo;s <a href=https://marvl.stanford.edu/index.html target=_blank rel=noopener>MARVL</a> lab and <a href=https://aimi.stanford.edu/ target=_blank rel=noopener>AIMI</a> Center.</p><p>My research focuses on the intersection of multimodal and self-supervised
learning, and the application of these methods to improve
healthcare.</p></div></div></div></div></section><section id=featured class="home-section wg-collection"><div class=home-section-bg></div><div class=container><div class=row><div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start"><h1 class=mb-0>Featured Publications</h1></div><div class="col-12 col-lg-8"><div class="card-simple view-card"><div class=article-metadata><div><span>Yuhui Zhang</span>, <span>Jeff Z HaoChen</span>, <span class=author-highlighted>Mars (Shih-Cheng) Huang</span>, <span>Kuan-Chieh Wang</span>, <span>James Zou</span>, <span>Serena Yeung</span></div><span class=article-date>January, 2023</span>
<span class=middot-divider></span>
<span class=pub-publication>ICLR</span></div><a href=/publication/dr-ml/><div class=img-hover-zoom><img src=/publication/dr-ml/featured_hub8739e2f7e68d2897bf9a51248f5d175_382214_808x455_fill_q75_h2_lanczos_smart1_3.webp height=455 width=808 class=article-banner alt="DrML: Diagnosing and Rectifying Vision Models using Language" loading=lazy></div></a><div class="section-subheading article-title mb-1 mt-3"><a href=/publication/dr-ml/>DrML: Diagnosing and Rectifying Vision Models using Language</a></div><a href=/publication/dr-ml/ class=summary-link><div class=article-style><p>The traditional process of diagnosing model behaviors in deployment settings involves labor-intensive data acquisition and annotation. Our proposed method, DrML, can discover high-error data slices, identify influential attributes and further rectify undesirable model behaviors, without requiring any visual data. Through a combination of theoretical explanation and empirical verification, we present conditions under which classifiers trained on embeddings from one modality can be equivalently applied to embeddings from another modality.</p></div></a><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://openreview.net/forum?id=losu6IAaPeB" target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/dr-ml/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/yuhui-zh15/model_audit target=_blank rel=noopener>Code</a></div></div><div class="card-simple view-card"><div class=article-metadata><div><span class=author-highlighted>Mars (Shih-Cheng) Huang</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Anuj Pareek</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Malte Jensen</span>, <span>Matthew P. Lungren</span>, <span>Serena Yeung</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Akshay S. Chaudhari</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i></div><span class=article-date>January, 2023</span>
<span class=middot-divider></span>
<span class=pub-publication>Nature Digital Medicine (Under Review)</span></div><a href=/publication/ssl/><div class=img-hover-zoom><img src=/publication/ssl/featured_hu7c5ff2c1e9ad51f00bd6875a80ad2826_468879_808x455_fill_q75_h2_lanczos_smart1_3.webp height=455 width=808 class=article-banner alt="Self-supervised learning for medical image classification: a systematic review and implementation guidelines" loading=lazy></div></a><div class="section-subheading article-title mb-1 mt-3"><a href=/publication/ssl/>Self-supervised learning for medical image classification: a systematic review and implementation guidelines</a></div><a href=/publication/ssl/ class=summary-link><div class=article-style><p>In this review, we provide consistent descriptions of different self-supervised learning strategies and compose a systematic review of papers published between 2012 and 2022 on PubMed, Scopus, and ArXiv that applied self-supervised learning to medical imaging classification. With this comprehensive effort, we synthesize the collective knowledge of prior work and provide implementation guidelines for future researchers interested in applying self-supervised learning to their development of medical imaging classification models.</p></div></a><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=/publication/ssl/conference-paper.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/ssl/cite.bib>Cite</a></div></div><div class="card-simple view-card"><div class=article-metadata><div><span class=author-highlighted>Mars (Shih-Cheng) Huang</span>, <span>Akshay S. Chaudhari</span>, <span>Curtis P. Langlotz</span>, <span>Nigam Shah</span>, <span>Serena Yeung</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Matthew P. Lungren</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i></div><span class=article-date>November, 2022</span>
<span class=middot-divider></span>
<span class=pub-publication>Nature Communications</span></div><a href=/publication/infectious/><div class=img-hover-zoom><img src=/publication/infectious/featured_hu7e55216efe0083e82003e78abbe6d877_128440_808x455_fill_q75_h2_lanczos_smart1.webp height=455 width=808 class=article-banner alt="Developing medical imaging AI for emerging infectious diseases" loading=lazy></div></a><div class="section-subheading article-title mb-1 mt-3"><a href=/publication/infectious/>Developing medical imaging AI for emerging infectious diseases</a></div><a href=/publication/infectious/ class=summary-link><div class=article-style><p>In this review, we provide an evidence-based roadmap for how machine learning technologies in medical imaging can be used to battle ongoing and future pandemics. Specifically, we focus in each section on the four most pressing issues, namely - needfinding, dataset curation, model development and subsequent evaluation, and post-deployment considerations.</p></div></a><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.nature.com/articles/s41467-022-34234-4 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/infectious/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/https://doi.org/10.1038/s41467-022-34234-4 target=_blank rel=noopener>DOI</a></div></div><div class="card-simple view-card"><div class=article-metadata><div><span>Yuyin Zhou</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span class=author-highlighted>Mars (Shih-Cheng) Huang</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Jason Alan Fries</span>, <span>Alaa Youssef</span>, <span>Timothy J. Amrhein</span>, <span>Marcello Chang</span>, <span>Imon Banerjee</span>, <span>Daniel Rubin</span>, <span>Lei Xing</span>, <span>Nigam Shah</span>, <span>Matthew P. Lungren</span></div><span class=article-date>November, 2021</span>
<span class=middot-divider></span>
<span class=pub-publication>arXiv</span></div><div class="section-subheading article-title mb-1 mt-3"><a href=/publication/radfusion/>RadFusion: Benchmarking Performance and Fairness for Multimodal Pulmonary Embolism Detection from CT and EHR</a></div><a href=/publication/radfusion/ class=summary-link><div class=article-style><p>We present RadFusion, a multimodal, benchmark dataset of 1794 patients with corresponding EHR data and high-resolution computed tomography (CT) scans labeled for pulmonary embolisms. We evaluate several representative multimodal fusion models and benchmark their fairness properties across protected subgroups, e.g., gender, race/ethnicity, age. Our results suggest that integrating imaging and EHR data can improve classification performance and robustness without introducing large disparities in the true positive rate between population groups.</p></div></a><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2111.11665 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/radfusion/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/https://doi.org/10.48550/arXiv.2111.11665 target=_blank rel=noopener>DOI</a></div></div><div class="card-simple view-card"><div class=article-metadata><div><span class=author-highlighted>Mars (Shih-Cheng) Huang</span>, <span>Liyue Shen</span>, <span>Matthew P Lungren</span>, <span>Serena Yeung</span></div><span class=article-date>October, 2021</span>
<span class=middot-divider></span>
<span class=pub-publication>ICCV</span></div><a href=/publication/gloria/><div class=img-hover-zoom><img src=/publication/gloria/_featured_hu7b103db7aaa772ca4060d19729a7e38e_204272_808x455_fill_q75_h2_lanczos_smart1_3.webp height=455 width=808 class=article-banner alt="GLoRIA: A Multimodal Global-Local Representation Learning Framework for Label-Efficient Medical Image Recognition" loading=lazy></div></a><div class="section-subheading article-title mb-1 mt-3"><a href=/publication/gloria/>GLoRIA: A Multimodal Global-Local Representation Learning Framework for Label-Efficient Medical Image Recognition</a></div><a href=/publication/gloria/ class=summary-link><div class=article-style><p>The purpose of this work is to develop label-efficient multimodal medical imaging representations by leveraging radiology reports. Specifically, we propose an attention-based framework (GLoRIA) for learning global and local representations by contrasting image sub-regions and words in the paired report. In addition, we propose methods to leverage the learned representations for various downstream medical image recognition tasks with limited labels.</p></div></a><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://openaccess.thecvf.com/content/ICCV2021/html/Huang_GLoRIA_A_Multimodal_Global-Local_Representation_Learning_Framework_for_Label-Efficient_Medical_ICCV_2021_paper.html target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/gloria/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/marshuang80/gloria target=_blank rel=noopener>Code</a></div></div><div class="card-simple view-card"><div class=article-metadata><div><span class=author-highlighted>Mars (Shih-Cheng) Huang</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Anuj Pareek</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Saeed Seyyedi</span>, <span>Imon Banerjee</span>, <span>Matthew P. Lungren</span></div><span class=article-date>October, 2020</span>
<span class=middot-divider></span>
<span class=pub-publication>Nature Digital Medicine</span></div><a href=/publication/multimodal/><div class=img-hover-zoom><img src=/publication/multimodal/featured_hudd148ad42da73487dd1d542f2b5fc2cf_38324_808x455_fill_q75_h2_lanczos_smart1.webp height=455 width=808 class=article-banner alt="Fusion of medical imaging and electronic health records using deep learning: a systematic review and implementation guidelines" loading=lazy></div></a><div class="section-subheading article-title mb-1 mt-3"><a href=/publication/multimodal/>Fusion of medical imaging and electronic health records using deep learning: a systematic review and implementation guidelines</a></div><a href=/publication/multimodal/ class=summary-link><div class=article-style><p>In this reivew, we describe different data fusion techniques that can be applied to combine medical imaging with EHR, and systematically review medical data fusion literature published between 2012 and 2020. By means of this systematic review, we present current knowledge, summarize important results and provide implementation guidelines to serve as a reference for researchers interested in the application of multimodal fusion in medical imaging.</p></div></a><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.nature.com/articles/s41746-020-00341-z target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/multimodal/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/https://doi.org/10.1038/s41746-020-00341-z target=_blank rel=noopener>DOI</a></div></div><div class="card-simple view-card"><div class=article-metadata><div><span>Anirudh Joshi</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Sabri Eyuboglu</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span class=author-highlighted>Mars (Shih-Cheng) Huang</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Jared Dunnmon</span>, <span>Arjun Soin</span>, <span>Guido Davidzon</span>, <span>Akshay Chaudhari</span>, <span>Matthew P Lungren</span></div><span class=article-date>April, 2020</span>
<span class=middot-divider></span>
<span class=pub-publication>arXiv</span></div><div class="section-subheading article-title mb-1 mt-3"><a href=/publication/onconet/>OncoNet: Weakly Supervised Siamese Network to automate cancer treatment response assessment between longitudinal FDG PET/CT examinations</a></div><a href=/publication/onconet/ class=summary-link><div class=article-style><p>In this work we develop onconet, novel machine learning algorithm that assesses treatment response from a 1,954 pairs of sequential fdg pet/ct exams through weak supervision using the standard uptake values (suvmax) in associated radiology reports. onconet demonstrates an auroc of 0.86 and 0.84 on internal and external institution test sets respectively for determination of change between scans while also showing strong agreement to clinical scoring systems with a kappa score of 0.8.</p></div></a><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2108.02016 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/onconet/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/https://doi.org/10.48550/arXiv.2108.02016 target=_blank rel=noopener>DOI</a></div></div><div class=see-all><a href=/publication/>See all publications
<i class="fas fa-angle-right"></i></a></div></div></div></div></section><section id=publications class="home-section wg-collection"><div class=home-section-bg></div><div class=container><div class=row><div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start"><h1 class=mb-0>All Publications</h1></div><div class="col-12 col-lg-8"><div class="alert alert-note"><div>Quickly discover relevant content by <a href=./publication/>filtering publications</a>.</div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yuhui Zhang</span>, <span>Jeff Z HaoChen</span>, <span class=author-highlighted>Mars (Shih-Cheng) Huang</span>, <span>Kuan-Chieh Wang</span>, <span>James Zou</span>, <span>Serena Yeung</span></span>
(2023).
<a href=/publication/dr-ml/>DrML: Diagnosing and Rectifying Vision Models using Language</a>.
ICLR.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://openreview.net/forum?id=losu6IAaPeB" target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/dr-ml/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/yuhui-zh15/model_audit target=_blank rel=noopener>Code</a></p></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Rui Yan</span>, <span>Liangqiong Qu</span>, <span>Qingyue Wei</span>, <span class=author-highlighted>Mars (Shih-Cheng) Huang</span>, <span>Liyue Shen</span>, <span>Daniel Rubin</span>, <span>Lei Xing</span>, <span>Yuyin Zhou</span></span>
(2023).
<a href=/publication/ssl-federated/>Self-supervised learning for medical image classification: a systematic review and implementation guidelines</a>.
IEEE Transactions on Medical Imaging.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10004993" target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/ssl-federated/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/rui-yan/SSL-FL target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/https://doi.org/10.1109/TMI.2022.3233574 target=_blank rel=noopener>DOI</a></p></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span class=author-highlighted>Mars (Shih-Cheng) Huang</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Anuj Pareek</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Malte Jensen</span>, <span>Matthew P. Lungren</span>, <span>Serena Yeung</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Akshay S. Chaudhari</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i></span>
(2023).
<a href=/publication/ssl/>Self-supervised learning for medical image classification: a systematic review and implementation guidelines</a>.
Nature Digital Medicine (Under Review).<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=/publication/ssl/conference-paper.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/ssl/cite.bib>Cite</a></p></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yuhui Zhang</span>, <span class=author-highlighted>Mars (Shih-Cheng) Huang</span>, <span>Zhengping Zhou,</span>, <span>Matthew P. Lungren</span>, <span>Serena Yeung</span></span>
(2022).
<a href=/publication/transformers-segmentation/>Adapting pre-trained vision transformers from 2D to 3D through weight inflation improves medical image segmentation</a>.
ML4H.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://proceedings.mlr.press/v193/zhang22a.html target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/transformers-segmentation/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/yuhui-zh15/TransSeg target=_blank rel=noopener>Code</a></p></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span class=author-highlighted>Mars (Shih-Cheng) Huang</span>, <span>Akshay S. Chaudhari</span>, <span>Curtis P. Langlotz</span>, <span>Nigam Shah</span>, <span>Serena Yeung</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Matthew P. Lungren</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i></span>
(2022).
<a href=/publication/infectious/>Developing medical imaging AI for emerging infectious diseases</a>.
Nature Communications.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.nature.com/articles/s41467-022-34234-4 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/infectious/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/https://doi.org/10.1038/s41467-022-34234-4 target=_blank rel=noopener>DOI</a></p></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Judy Wawira Gichoya</span>, <span>Imon Banerjee</span>, <span>Ananth Reddy Bhimireddy</span>, <span>John L Burns</span>, <span>Leo Anthony Celi</span>, <span>Li-Ching Chen</span>, <span>Ramon Correa</span>, <span>Natalie Dullerud</span>, <span>Marzyeh Ghassemi</span>, <span class=author-highlighted>Mars (Shih-Cheng) Huang</span>, <span>Po-Chih Kuo</span>, <span>Matthew P Lungren</span>, <span>Lyle J Palmer</span>, <span>Brandon J Price</span>, <span>Saptarshi Purkayastha</span>, <span>Ayis T Pyrros</span>, <span>Lauren Oakden-Rayner</span>, <span>Chima Okechukwu</span>, <span>Laleh Seyyed-Kalantari</span>, <span>Hari Trivedi</span>, <span>Ryan Wang</span>, <span>Zachary Zaiman</span>, <span>Haoran Zhang</span></span>
(2022).
<a href=/publication/ai_race/>AI recognition of patient race in medical imaging: a modelling study</a>.
The Lancet Digital Health.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.sciencedirect.com/science/article/pii/S2589750022000632 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/ai_race/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/Emory-HITI/AI-Vengers target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/https://doi.org/10.1016/S2589-7500%2822%2900063-2 target=_blank rel=noopener>DOI</a></p></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Andre Esteva</span>, <span>Jean Feng</span>, <span>Douwe van der Wal</span>, <span class=author-highlighted>Mars (Shih-Cheng) Huang</span>, <span>Jeffry P Simko</span>, <span>Sandy DeVries</span>, <span>Emmalyn Chen</span>, <span>Edward M Schaeffer</span>, <span>Todd M Morgan</span>, <span>Yilun Sun</span>, <span>Amirata Ghorbani</span>, <span>Nikhil Naik</span>, <span>Dhruv Nathawani</span>, <span>Richard Socher</span>, <span>Jeff M Michalski</span>, <span>Mack Roach III</span>, <span>Thomas M Pisansky</span>, <span>Jedidiah M Monson</span>, <span>Farah Naz</span>, <span>James Wallace</span>, <span>Michelle J Ferguson</span>, <span>Jean-Paul Bahary</span>, <span>James Zou</span>, <span>Matthew Lungren</span>, <span>Serena Yeung</span>, <span>Ashley E Ross</span>, <span>Howard M Sandler</span>, <span>Phuoc T Tran</span>, <span>Daniel E Spratt</span>, <span>Stephanie Pugh</span>, <span>Felix Y Feng</span>, <span>Osama Mohamad</span>, <span>NRG Prostate Cancer AI Consortium</span></span>
(2022).
<a href=/publication/prostate_multimodal/>Prostate cancer therapy personalization via multi-modal deep learning on randomized phase III clinical trials</a>.
Nature Digital Medicine.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.nature.com/articles/s41746-022-00613-w target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/prostate_multimodal/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/https://doi.org/10.1038/s41746-022-00613-w target=_blank rel=noopener>DOI</a></p></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Andre Esteva</span>, <span>Jean Feng</span>, <span class=author-highlighted>Mars (Shih-Cheng) Huang</span>, <span>Douwe van der Wal</span>, <span>Jeffry Simko</span>, <span>Sandy DeVries</span>, <span>Emmalyn Chen</span>, <span>Edward M Schaeffer</span>, <span>Todd Matthew Morgan</span>, <span>Jedidiah Mercer Monson</span>, <span>Farah Naz</span>, <span>James Wallace</span>, <span>Michelle J Ferguson</span>, <span>Jean-Paul Bahary</span>, <span>Howard M Sandler</span>, <span>Phuoc T Tran</span>, <span>Daniel Eidelberg Spratt</span>, <span>Stephanie L Pugh</span>, <span>Felix Y Feng</span>, <span>Osama Mohamad</span></span>
(2022).
<a href=/publication/prostate_multimodal_oncology/>Development and validation of a prognostic AI biomarker using multi-modal deep learning with digital histopathology in localized prostate cancer on NRG Oncology phase III clinical trials.</a>.
Journal of Clinical Oncology.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://ascopubs.org/doi/abs/10.1200/JCO.2022.40.6_suppl.222 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/prostate_multimodal_oncology/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/https://doi.org/10.1200/JCO.2022.40.6_suppl.222 target=_blank rel=noopener>DOI</a></p></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Jiangdian Song</span>, <span class=author-highlighted>Mars (Shih-Cheng) Huang</span>, <span>Brendan Kelly</span>, <span>Guanqun Liao</span>, <span>Jing-yun Shi</span>, <span>Ning Wu</span>, <span>Weimin Li</span>, <span>Zaiyi Liu</span>, <span>Lei Cui</span>, <span>Matthew P Lungren</span>, <span>Michael Moseley</span>, <span>Peng Gao</span>, <span>Jie Tian</span>, <span>Kristen Yeom</span></span>
(2021).
<a href=/publication/lung_segmentation/>Automatic lung nodule segmentation and intra-nodular heterogeneity image generation</a>.
IEEE Journal of Biomedical and Health Informatics.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://ieeexplore.ieee.org/abstract/document/9652062 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/lung_segmentation/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/https://doi.org/10.1109/JBHI.2021.3135647 target=_blank rel=noopener>DOI</a></p></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yuyin Zhou</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span class=author-highlighted>Mars (Shih-Cheng) Huang</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Jason Alan Fries</span>, <span>Alaa Youssef</span>, <span>Timothy J. Amrhein</span>, <span>Marcello Chang</span>, <span>Imon Banerjee</span>, <span>Daniel Rubin</span>, <span>Lei Xing</span>, <span>Nigam Shah</span>, <span>Matthew P. Lungren</span></span>
(2021).
<a href=/publication/radfusion/>RadFusion: Benchmarking Performance and Fairness for Multimodal Pulmonary Embolism Detection from CT and EHR</a>.
arXiv.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2111.11665 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/radfusion/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/https://doi.org/10.48550/arXiv.2111.11665 target=_blank rel=noopener>DOI</a></p></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span class=author-highlighted>Mars (Shih-Cheng) Huang</span>, <span>Liyue Shen</span>, <span>Matthew P Lungren</span>, <span>Serena Yeung</span></span>
(2021).
<a href=/publication/gloria/>GLoRIA: A Multimodal Global-Local Representation Learning Framework for Label-Efficient Medical Image Recognition</a>.
ICCV.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://openaccess.thecvf.com/content/ICCV2021/html/Huang_GLoRIA_A_Multimodal_Global-Local_Representation_Learning_Framework_for_Label-Efficient_Medical_ICCV_2021_paper.html target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/gloria/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/marshuang80/gloria target=_blank rel=noopener>Code</a></p></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span class=author-highlighted>Mars (Shih-Cheng) Huang</span>, <span>Anuj Pareek</span>, <span>Roham Zamanian</span>, <span>Imon Banerjee</span>, <span>Matthew P. Lungren</span></span>
(2020).
<a href=/publication/pe_multimodal/>Multimodal fusion with deep neural networks for leveraging CT imaging and electronic health record: a case-study in pulmonary embolism detection</a>.
Nature Scientific Reports.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.nature.com/articles/s41598-020-78888-w target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/pe_multimodal/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/https://doi.org/10.1038/s41598-020-78888-w target=_blank rel=noopener>DOI</a></p></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Ashton Teng</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Blanca Villanueva</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Derek Jow</span>, <span class=author-highlighted>Mars (Shih-Cheng) Huang</span>, <span>Samantha N Piekos</span>, <span>Russ B Altman</span></span>
(2020).
<a href=/publication/bgv/>Biomedical Graph Visualizer for Identifying Drug Candidates</a>.
biorXiv.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.biorxiv.org/content/10.1101/2020.11.27.368811v1.abstract target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/bgv/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/https://doi.org/10.1101/2020.11.27.368811 target=_blank rel=noopener>DOI</a></p></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span class=author-highlighted>Mars (Shih-Cheng) Huang</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Anuj Pareek</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Saeed Seyyedi</span>, <span>Imon Banerjee</span>, <span>Matthew P. Lungren</span></span>
(2020).
<a href=/publication/multimodal/>Fusion of medical imaging and electronic health records using deep learning: a systematic review and implementation guidelines</a>.
Nature Digital Medicine.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.nature.com/articles/s41746-020-00341-z target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/multimodal/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/https://doi.org/10.1038/s41746-020-00341-z target=_blank rel=noopener>DOI</a></p></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Anirudh Joshi</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Sabri Eyuboglu</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span class=author-highlighted>Mars (Shih-Cheng) Huang</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Jared Dunnmon</span>, <span>Arjun Soin</span>, <span>Guido Davidzon</span>, <span>Akshay Chaudhari</span>, <span>Matthew P Lungren</span></span>
(2020).
<a href=/publication/onconet/>OncoNet: Weakly Supervised Siamese Network to automate cancer treatment response assessment between longitudinal FDG PET/CT examinations</a>.
arXiv.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2108.02016 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/onconet/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/https://doi.org/10.48550/arXiv.2108.02016 target=_blank rel=noopener>DOI</a></p></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span class=author-highlighted>Mars (Shih-Cheng) Huang</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Tanay Kothari</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Imon Banerjee</span>, <span>Chris Chute</span>, <span>Robyn L Ball</span>, <span>Norah Borus</span>, <span>Andrew Huang</span>, <span>Bhavik N Patel</span>, <span>Pranav Rajpurkar</span>, <span>Jeremy Irvin</span>, <span>Jared Dunnmon</span>, <span>Joseph Bledsoe</span>, <span>Katie Shpanskaya</span>, <span>Abhay Dhaliwal</span>, <span>Roham Zamanian</span>, <span>Andrew Y Ng</span>, <span>Matthew P Lungren</span></span>
(2020).
<a href=/publication/penet/>PENetâ€”a scalable deep-learning model for automated diagnosis of pulmonary embolism using volumetric CT imaging</a>.
Nature Digital Medicine.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.nature.com/articles/s41746-020-0266-y target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/penet/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/https://doi.org/10.1038/s41746-020-00310-6 target=_blank rel=noopener>DOI</a></p></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Adam Rule</span>, <span>Amanda Birmingham</span>, <span>Cristal Zuniga</span>, <span>Ilkay Altintas</span>, <span class=author-highlighted>Mars (Shih-Cheng) Huang</span>, <span>Rob Knight</span>, <span>Niema Moshiri</span>, <span>Mai H. Nguyen</span>, <span>Sara Brin Rosenthal</span>, <span>Fernando PÃ©rez</span>, <span>Peter W. Rose</span></span>
(2019).
<a href=/publication/ten_rules/>Ten simple rules for writing and sharing computational analyses in Jupyter Notebooks</a>.
PLOS Computational Biology.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007007&ref=https://githubhelp.com" target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/ten_rules/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/https://doi.org/10.1371/journal.pcbi.1007007 target=_blank rel=noopener>DOI</a></p></div></div></div></div></section><section id=experiences class="home-section wg-experience"><div class=home-section-bg></div><div class=container><div class=row><div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start"><h1 class=mb-0>Experience</h1></div><div class="col-12 col-lg-8"><div class="row experience"><div class="col-auto text-center flex-column d-none d-sm-flex"><div class="row h-50"><div class=col>&nbsp;</div><div class=col>&nbsp;</div></div><div class=m-2><span class="badge badge-pill border">&nbsp;</span></div><div class="row h-50"><div class="col border-right">&nbsp;</div><div class=col>&nbsp;</div></div></div><div class="col py-2"><div class=card><div class=card-body><div class="d-flex align-content-start"><div class="mr-2 mb-2"><a href=https://www.salesforceairesearch.com/ target=_blank rel=noopener><img src=/media/icons/brands/org-s.svg width=56px height=56px alt=Salesforce loading=lazy></a></div><div><div class="section-subheading card-title exp-title text-muted my-0">AI Research Summer Intern</div><div class="section-subheading card-title exp-company text-muted my-0"><a href=https://www.salesforceairesearch.com/ target=_blank rel=noopener>Salesforce</a></div><div class="text-muted exp-meta">June 2021 â€“
September 2021
<span class=middot-divider></span>
<span>San Francisco</span></div></div></div><div class=card-text><p>Designed and implemented a multimodal self-supervised framework for prostate cancer long-term outcome prediction.</p><p><figure id=figure-the-multimodal-architecture-is-composed-of-two-parts-a-stack-to-parse-a-variable-number-of-digital-histopathology-slides-and-another-stack-to-merge-the-resultant-features-and-predict-binary-long-term-outcomes><div class="d-flex justify-content-center"><div class=w-100><img src=./publication/prostate_multimodal_oncology/featured.jpeg alt="screen reader text" loading=lazy data-zoomable></div></div><figcaption>The multimodal architecture is composed of two parts: a stack to parse a variable number of digital histopathology slides and another stack to merge the resultant features and predict binary long-term outcomes.</figcaption></figure></p><p><a href=https://www.nature.com/articles/s41746-022-00613-w target=_blank rel=noopener>Project Link</a></p></div></div></div></div></div><div class="row experience"><div class="col-auto text-center flex-column d-none d-sm-flex"><div class="row h-50"><div class="col border-right">&nbsp;</div><div class=col>&nbsp;</div></div><div class=m-2><span class="badge badge-pill border">&nbsp;</span></div><div class="row h-50"><div class="col border-right">&nbsp;</div><div class=col>&nbsp;</div></div></div><div class="col py-2"><div class=card><div class=card-body><div class="d-flex align-content-start"><div class="mr-2 mb-2"><a href=https://chanzuckerberg.com/ target=_blank rel=noopener><img src=/media/icons/brands/org-czi.svg width=56px height=56px alt="Chan Zuckerberg Initiative (CZI)" loading=lazy></a></div><div><div class="section-subheading card-title exp-title text-muted my-0">Computational Biology Summer Intern</div><div class="section-subheading card-title exp-company text-muted my-0"><a href=https://chanzuckerberg.com/ target=_blank rel=noopener>Chan Zuckerberg Initiative (CZI)</a></div><div class="text-muted exp-meta">June 2019 â€“
September 2019
<span class=middot-divider></span>
<span>Redwood City</span></div></div></div><div class=card-text><p>Created Segmentify, an interactive and general-purpose cell segmentation plugin for the image viewer Napari</p><p><figure id=figure-in-the-example-above-the-user-is-using-segmentify-to-segment-out-the-nucleus-cytoplasm-and-the-background-for-all-the-cells-in-the-image-the-trained-classifier-is-used-to-predict-the-label-for-all-the-remaining-unlabeled-pixels-and-the-segmentation-output-is-displayed-at-the-segmentation-labels-layer><div class="d-flex justify-content-center"><div class=w-100><img src=https://raw.githubusercontent.com/transformify-plugins/segmentify/master/figs/segmentify.gif alt="screen reader text" loading=lazy data-zoomable></div></div><figcaption>In the example above, the user is using segmentify to segment out the nucleus, cytoplasm and the background for all the cells in the image. The trained classifier is used to predict the label for all the remaining unlabeled pixels, and the segmentation output is displayed at the segmentation labels layer.</figcaption></figure></p><p><a href=https://github.com/transformify-plugins/segmentify target=_blank rel=noopener>Project Link</a></p></div></div></div></div></div><div class="row experience"><div class="col-auto text-center flex-column d-none d-sm-flex"><div class="row h-50"><div class="col border-right">&nbsp;</div><div class=col>&nbsp;</div></div><div class=m-2><span class="badge badge-pill border">&nbsp;</span></div><div class="row h-50"><div class=col>&nbsp;</div><div class=col>&nbsp;</div></div></div><div class="col py-2"><div class=card><div class=card-body><div class="d-flex align-content-start"><div class="mr-2 mb-2"><a href=https://www.sdsc.edu/ target=_blank rel=noopener><img src=/media/icons/brands/org-sdsc.svg width=56px height=56px alt="San Diego Supercomputer Center" loading=lazy></a></div><div><div class="section-subheading card-title exp-title text-muted my-0">Research Programmer</div><div class="section-subheading card-title exp-company text-muted my-0"><a href=https://www.sdsc.edu/ target=_blank rel=noopener>San Diego Supercomputer Center</a></div><div class="text-muted exp-meta">March 2015 â€“
December 2016
<span class=middot-divider></span>
<span>San Diego</span></div></div></div><div class=card-text><p>Developed mmtf-pyspark, a python package that parallelizes analysis and mining of protein data using Apache-Spark</p><p><figure id=figure-visualization-of-protein-dna-complex-using-mmtf-pyspark><div class="d-flex justify-content-center"><div class=w-100><img src=https://mmtf-pyspark.readthedocs.io/en/latest/_images/ProteinDnaComplex.png alt="screen reader text" loading=lazy data-zoomable></div></div><figcaption>Visualization of Protein-DNA complex using MMTF-PySpark.</figcaption></figure></p><p><a href=https://github.com/sbl-sdsc/mmtf-pyspark target=_blank rel=noopener>Project Link</a></p></div></div></div></div></div></div></div></div></section><section id=photos class="home-section wg-markdown"><div class=home-section-bg></div><div class=container><div class="row justify-content-center"><div class="section-heading col-12 mb-3 text-center"><h1 class=mb-0>Amature Photography</h1></div><div class=col-12><div class=gallery-grid><div class="gallery-item gallery-item--medium"><a data-fancybox=gallery-demo href=/media/albums/demo/-B3A1944.jpg><img src=/media/albums/demo/-B3A1944_hu1e5040d05941b7da612fad3b1aec666d_2003007_750x750_fit_q75_h2_lanczos.webp loading=lazy alt=-B3A1944.jpg width=750 height=500></a></div><div class="gallery-item gallery-item--medium"><a data-fancybox=gallery-demo href=/media/albums/demo/-B3A7969.jpg><img src=/media/albums/demo/-B3A7969_hu1992682bd1aca7c30460a0950270ee84_2700756_750x750_fit_q75_h2_lanczos.webp loading=lazy alt=-B3A7969.jpg width=750 height=500></a></div><div class="gallery-item gallery-item--medium"><a data-fancybox=gallery-demo href=/media/albums/demo/-IMG-7432.jpg><img src=/media/albums/demo/-IMG-7432_huefd3b67b5306ba0f1392bddc338f8419_1355864_750x750_fit_q75_h2_lanczos.webp loading=lazy alt=-IMG-7432.jpg width=750 height=500></a></div><div class="gallery-item gallery-item--medium"><a data-fancybox=gallery-demo href=/media/albums/demo/B28624D3-3423-4D40-87B0-C162E7F17AE3.jpg><img src=/media/albums/demo/B28624D3-3423-4D40-87B0-C162E7F17AE3_hu66f0ee6f4771a2fa594d7b3838d6a1ae_770917_750x750_fit_q75_h2_lanczos.webp loading=lazy alt=B28624D3-3423-4D40-87B0-C162E7F17AE3.jpg width=422 height=750></a></div><div class="gallery-item gallery-item--medium"><a data-fancybox=gallery-demo href=/media/albums/demo/IMG-0213.jpg><img src=/media/albums/demo/IMG-0213_hu41dc51d4cb1843c7dbe60b29acdd6e8f_6006845_750x750_fit_q75_h2_lanczos.webp loading=lazy alt=IMG-0213.jpg width=563 height=750></a></div><div class="gallery-item gallery-item--medium"><a data-fancybox=gallery-demo href=/media/albums/demo/IMG-0315.JPG><img src=/media/albums/demo/IMG-0315_hu59b63af5c6827743cf8434b24fdfb469_1135871_750x750_fit_q75_h2_lanczos.webp loading=lazy alt=IMG-0315.JPG width=563 height=750></a></div><div class="gallery-item gallery-item--medium"><a data-fancybox=gallery-demo href=/media/albums/demo/IMG-0323.jpg><img src=/media/albums/demo/IMG-0323_hua5ac560a90dd55d0edf2e88325d832ce_2082913_750x750_fit_q75_h2_lanczos.webp loading=lazy alt=IMG-0323.jpg width=563 height=750></a></div><div class="gallery-item gallery-item--medium"><a data-fancybox=gallery-demo href=/media/albums/demo/IMG-0917%20%281%29.jpg><img src=/media/albums/demo/IMG-0917%20%281%29_hubc01cdfa2006104f5410f939b5140984_558096_750x750_fit_q75_h2_lanczos.webp loading=lazy alt="IMG-0917 (1).jpg" width=569 height=750></a></div><div class="gallery-item gallery-item--medium"><a data-fancybox=gallery-demo href=/media/albums/demo/IMG-3482.jpg><img src=/media/albums/demo/IMG-3482_hu89429714d54a944b3406ccbba9587ef8_93337_750x750_fit_q75_h2_lanczos.webp loading=lazy alt=IMG-3482.jpg width=750 height=500></a></div><div class="gallery-item gallery-item--medium"><a data-fancybox=gallery-demo href=/media/albums/demo/IMG-3928.JPG><img src=/media/albums/demo/IMG-3928_hu54db6c8ec03dc71b21d078d3ea9472ea_166588_750x750_fit_q75_h2_lanczos.webp loading=lazy alt=IMG-3928.JPG width=500 height=750></a></div><div class="gallery-item gallery-item--medium"><a data-fancybox=gallery-demo href=/media/albums/demo/IMG-4038.JPG><img src=/media/albums/demo/IMG-4038_hu81078be0c19a7a44323bee18a0c011d8_502361_750x750_fit_q75_h2_lanczos.webp loading=lazy alt=IMG-4038.JPG width=750 height=500></a></div><div class="gallery-item gallery-item--medium"><a data-fancybox=gallery-demo href=/media/albums/demo/IMG-7175.JPG><img src=/media/albums/demo/IMG-7175_hu1104aa2863da19e197ce36a76b267dd5_1169097_750x750_fit_q75_h2_lanczos.webp loading=lazy alt=IMG-7175.JPG width=500 height=750></a></div></div></div></div></div></section></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">Â© 2023 Me. This work is licensed under <a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank>CC BY NC ND 4.0</a></p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> â€” the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.d26509351aa0ff874abbee824e982e9b.js></script>
<script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":false}</script><script src=/en/js/wowchemy.min.e8ee06ba8371980ffde659871dd593b0.js></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js type=module></script></body></html>